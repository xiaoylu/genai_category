{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:01<00:00, 8372259.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 508706.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 3812462.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),           # Convert PIL image to tensor\n",
    "    transforms.Normalize((0.,), (0.5,))  # Normalize grayscale values to [-1, 1] if desired\n",
    "])\n",
    "\n",
    "mnist_train = MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    mnist_train,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # input is BCHW\n",
    "        diffY = x2.size()[-1] - x1.size()[-1]\n",
    "        diffX = x1.size()[-2] - x1.size()[-2]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class EmbeddedFC(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.Linear(out_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t_embed = self.fc(t)\n",
    "        return t_embed\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        \n",
    "        self.up1 = (Up(256, 128))\n",
    "        self.up2 = (Up(128, 64))\n",
    "        self.outc = nn.Sequential(\n",
    "            nn.Conv2d(64, n_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.t_embed1 = EmbeddedFC(1, 256)\n",
    "        self.t_embed2 = EmbeddedFC(1, 128)\n",
    "        \n",
    "        self.c_embed1 = EmbeddedFC(10, 256)\n",
    "        self.c_embed2 = EmbeddedFC(10, 128)\n",
    "        \n",
    "    def forward(self, x, t, c):\n",
    "        # x=[B,n_channel,H,W]\n",
    "        # t=[B,1]\n",
    "        \n",
    "        x1 = self.inc(x) # out=[B, 64, H, W]\n",
    "        x2 = self.down1(x1) # out=[B, 128, H/2, W/2]\n",
    "        x3 = self.down2(x2) # out=[B, 256, H/4, W/4]\n",
    "        \n",
    "        t1 = self.t_embed1(t) # out=[B, 256]\n",
    "        t1 = t1.view(-1, 256, 1, 1).expand(-1, 256, x3.shape[-2], x3.shape[-1]) # out=[B, 256, H/4, W/4]\n",
    "        c = F.one_hot(c, num_classes=10).type(torch.float) # [B, 10]\n",
    "        c1 = self.c_embed1(c) # out=[B, 256]\n",
    "        c1 = c1.view(-1, 256, 1, 1).expand(-1, 256, x3.shape[-2], x3.shape[-1]) # out=[B, 256, H/4, W/4]\n",
    "        x = self.up1(x3 * c1 + t1, x2) # out=[B, 128, H/2, W/2]\n",
    "\n",
    "        t2 = self.t_embed2(t) # out=[B, 128]\n",
    "        t2 = t2.view(-1, 128, 1, 1).expand(-1, 128, x.shape[-2], x.shape[-1]) # out=[B, 128, H/2, W/2]\n",
    "        c2 = self.c_embed2(c) # out=[B, 128]\n",
    "        c2 = c2.view(-1, 128, 1, 1).expand(-1, 128, x.shape[-2], x.shape[-1]) # out=[B, 256, H/4, W/4]\n",
    "        x = self.up2(x * c2 + t2, x1) # out=[B, 64, H, W]\n",
    "        logits = self.outc(x) # out=[B, n_channel, H, W]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = UNet(n_channels=1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000  # total timesteps\n",
    "betas = torch.linspace(1e-4, 0.02, T)  # example linear schedule\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory=0.31 GB\n",
      "Epoch 0: Loss 241.6931013148278\n",
      "Memory=0.31 GB\n",
      "Epoch 1: Loss 36.45743010751903\n",
      "Memory=0.31 GB\n",
      "Epoch 2: Loss 31.089791741222143\n",
      "Memory=0.31 GB\n",
      "Epoch 3: Loss 28.898860129527748\n",
      "Memory=0.31 GB\n",
      "Epoch 4: Loss 27.161011239513755\n",
      "Memory=0.31 GB\n",
      "Epoch 5: Loss 26.433062057942152\n",
      "Memory=0.31 GB\n",
      "Epoch 6: Loss 25.589492649771273\n",
      "Memory=0.31 GB\n",
      "Epoch 7: Loss 25.53839249908924\n",
      "Memory=0.31 GB\n",
      "Epoch 8: Loss 24.617831382900476\n",
      "Memory=0.31 GB\n",
      "Epoch 9: Loss 24.58531565219164\n",
      "Memory=0.31 GB\n",
      "Epoch 10: Loss 24.052596031688154\n",
      "Memory=0.31 GB\n",
      "Epoch 11: Loss 23.70093209296465\n",
      "Memory=0.31 GB\n",
      "Epoch 12: Loss 23.84518222231418\n",
      "Memory=0.31 GB\n",
      "Epoch 13: Loss 23.640151174739003\n",
      "Memory=0.31 GB\n",
      "Epoch 14: Loss 23.25425104610622\n",
      "Memory=0.31 GB\n",
      "Epoch 15: Loss 23.028102171607316\n",
      "Memory=0.31 GB\n",
      "Epoch 16: Loss 23.171767900697887\n",
      "Memory=0.31 GB\n",
      "Epoch 17: Loss 23.049274628981948\n",
      "Memory=0.31 GB\n",
      "Epoch 18: Loss 22.66698209475726\n",
      "Memory=0.31 GB\n",
      "Epoch 19: Loss 22.60218847543001\n",
      "Memory=0.31 GB\n",
      "Epoch 20: Loss 22.537471318617463\n",
      "Memory=0.31 GB\n",
      "Epoch 21: Loss 22.52466938085854\n",
      "Memory=0.31 GB\n",
      "Epoch 22: Loss 22.25717963743955\n",
      "Memory=0.31 GB\n",
      "Epoch 23: Loss 22.434405392967165\n",
      "Memory=0.31 GB\n",
      "Epoch 24: Loss 22.231426891870797\n",
      "Memory=0.31 GB\n",
      "Epoch 25: Loss 22.110450067557395\n",
      "Memory=0.31 GB\n",
      "Epoch 26: Loss 22.04679825063795\n",
      "Memory=0.31 GB\n",
      "Epoch 27: Loss 22.092763197608292\n",
      "Memory=0.31 GB\n",
      "Epoch 28: Loss 22.07143785338849\n",
      "Memory=0.31 GB\n",
      "Epoch 29: Loss 21.760756061412394\n",
      "Memory=0.31 GB\n",
      "Epoch 30: Loss 22.139557381160557\n",
      "Memory=0.31 GB\n",
      "Epoch 31: Loss 21.849430095404387\n",
      "Memory=0.31 GB\n",
      "Epoch 32: Loss 22.074954755604267\n",
      "Memory=0.31 GB\n",
      "Epoch 33: Loss 21.74904922116548\n",
      "Memory=0.31 GB\n",
      "Epoch 34: Loss 21.858634289354086\n",
      "Memory=0.31 GB\n",
      "Epoch 35: Loss 21.632227947935462\n",
      "Memory=0.31 GB\n",
      "Epoch 36: Loss 21.6010682573542\n",
      "Memory=0.31 GB\n",
      "Epoch 37: Loss 21.71530858334154\n",
      "Memory=0.31 GB\n",
      "Epoch 38: Loss 21.571094284765422\n",
      "Memory=0.31 GB\n",
      "Epoch 39: Loss 21.512040955014527\n",
      "Memory=0.31 GB\n",
      "Epoch 40: Loss 21.591816841624677\n",
      "Memory=0.31 GB\n",
      "Epoch 41: Loss 21.421666130423546\n",
      "Memory=0.31 GB\n",
      "Epoch 42: Loss 21.41186999436468\n",
      "Memory=0.31 GB\n",
      "Epoch 43: Loss 21.312676335684955\n",
      "Memory=0.31 GB\n",
      "Epoch 44: Loss 21.46312704961747\n",
      "Memory=0.31 GB\n",
      "Epoch 45: Loss 21.54068574961275\n",
      "Memory=0.31 GB\n",
      "Epoch 46: Loss 21.241041044704616\n",
      "Memory=0.31 GB\n",
      "Epoch 47: Loss 21.193968741223216\n",
      "Memory=0.31 GB\n",
      "Epoch 48: Loss 21.539429277181625\n",
      "Memory=0.31 GB\n",
      "Epoch 49: Loss 21.239382254891098\n",
      "Memory=0.31 GB\n",
      "Epoch 50: Loss 21.138764340430498\n",
      "Memory=0.31 GB\n",
      "Epoch 51: Loss 21.193520015105605\n",
      "Memory=0.31 GB\n",
      "Epoch 52: Loss 21.102365635335445\n",
      "Memory=0.31 GB\n",
      "Epoch 53: Loss 21.092912090942264\n",
      "Memory=0.31 GB\n",
      "Epoch 54: Loss 21.078258426859975\n",
      "Memory=0.31 GB\n",
      "Epoch 55: Loss 21.09389823116362\n",
      "Memory=0.31 GB\n",
      "Epoch 56: Loss 21.29956455901265\n",
      "Memory=0.31 GB\n",
      "Epoch 57: Loss 20.972545788623393\n",
      "Memory=0.31 GB\n",
      "Epoch 58: Loss 20.96850004978478\n",
      "Memory=0.31 GB\n",
      "Epoch 59: Loss 20.716935481876135\n",
      "Memory=0.31 GB\n",
      "Epoch 60: Loss 21.01737631112337\n",
      "Memory=0.31 GB\n",
      "Epoch 61: Loss 21.114018122665584\n",
      "Memory=0.31 GB\n",
      "Epoch 62: Loss 20.960083024576306\n",
      "Memory=0.31 GB\n",
      "Epoch 63: Loss 21.030984190292656\n",
      "Memory=0.31 GB\n",
      "Epoch 64: Loss 21.04231953807175\n",
      "Memory=0.31 GB\n",
      "Epoch 65: Loss 20.889022808521986\n",
      "Memory=0.31 GB\n",
      "Epoch 66: Loss 20.886589565314353\n",
      "Memory=0.31 GB\n",
      "Epoch 67: Loss 20.73363897483796\n",
      "Memory=0.31 GB\n",
      "Epoch 68: Loss 20.705723549239337\n",
      "Memory=0.31 GB\n",
      "Epoch 69: Loss 20.90463998168707\n",
      "Memory=0.31 GB\n",
      "Epoch 70: Loss 21.0073078032583\n",
      "Memory=0.31 GB\n",
      "Epoch 71: Loss 20.957476688548923\n",
      "Memory=0.31 GB\n",
      "Epoch 72: Loss 20.829764852300286\n",
      "Memory=0.31 GB\n",
      "Epoch 73: Loss 20.65299630817026\n",
      "Memory=0.31 GB\n",
      "Epoch 74: Loss 20.52746535371989\n",
      "Memory=0.31 GB\n",
      "Epoch 75: Loss 20.741899078711867\n",
      "Memory=0.31 GB\n",
      "Epoch 76: Loss 20.545159166678786\n",
      "Memory=0.31 GB\n",
      "Epoch 77: Loss 20.63761806394905\n",
      "Memory=0.31 GB\n",
      "Epoch 78: Loss 20.53996647708118\n",
      "Memory=0.31 GB\n",
      "Epoch 79: Loss 20.63290004618466\n",
      "Memory=0.31 GB\n",
      "Epoch 80: Loss 20.65560207888484\n",
      "Memory=0.31 GB\n",
      "Epoch 81: Loss 20.622910764068365\n",
      "Memory=0.31 GB\n",
      "Epoch 82: Loss 20.610855330713093\n",
      "Memory=0.31 GB\n",
      "Epoch 83: Loss 20.86111089307815\n",
      "Memory=0.31 GB\n",
      "Epoch 84: Loss 20.713151743635535\n",
      "Memory=0.31 GB\n",
      "Epoch 85: Loss 20.639510283246636\n",
      "Memory=0.31 GB\n",
      "Epoch 86: Loss 20.505131683312356\n",
      "Memory=0.31 GB\n",
      "Epoch 87: Loss 20.496450719423592\n",
      "Memory=0.31 GB\n",
      "Epoch 88: Loss 20.622829765081406\n",
      "Memory=0.31 GB\n",
      "Epoch 89: Loss 20.401203422807157\n",
      "Memory=0.31 GB\n",
      "Epoch 90: Loss 20.569039820693433\n",
      "Memory=0.31 GB\n",
      "Epoch 91: Loss 20.56270743533969\n",
      "Memory=0.31 GB\n",
      "Epoch 92: Loss 20.590382670983672\n",
      "Memory=0.31 GB\n",
      "Epoch 93: Loss 20.444009205326438\n",
      "Memory=0.31 GB\n",
      "Epoch 94: Loss 20.399509222246706\n",
      "Memory=0.31 GB\n",
      "Epoch 95: Loss 20.455749491229653\n",
      "Memory=0.31 GB\n",
      "Epoch 96: Loss 20.637733532115817\n",
      "Memory=0.31 GB\n",
      "Epoch 97: Loss 20.468043294735253\n",
      "Memory=0.31 GB\n",
      "Epoch 98: Loss 20.506781038828194\n",
      "Memory=0.31 GB\n",
      "Epoch 99: Loss 20.620955342426896\n",
      "Memory=0.31 GB\n",
      "Epoch 100: Loss 20.194062599912286\n",
      "Memory=0.31 GB\n",
      "Epoch 101: Loss 20.319514237344265\n",
      "Memory=0.31 GB\n",
      "Epoch 102: Loss 20.373708341270685\n",
      "Memory=0.31 GB\n",
      "Epoch 103: Loss 20.269591418094933\n",
      "Memory=0.31 GB\n",
      "Epoch 104: Loss 20.30815188214183\n",
      "Memory=0.31 GB\n",
      "Epoch 105: Loss 20.355802685953677\n",
      "Memory=0.31 GB\n",
      "Epoch 106: Loss 20.26142186112702\n",
      "Memory=0.31 GB\n",
      "Epoch 107: Loss 20.305888728238642\n",
      "Memory=0.31 GB\n",
      "Epoch 108: Loss 20.38174912892282\n",
      "Memory=0.31 GB\n",
      "Epoch 109: Loss 20.388185357674956\n",
      "Memory=0.31 GB\n",
      "Epoch 110: Loss 20.469121518544853\n",
      "Memory=0.31 GB\n",
      "Epoch 111: Loss 20.22555706463754\n",
      "Memory=0.31 GB\n",
      "Epoch 112: Loss 20.247452751733363\n",
      "Memory=0.31 GB\n",
      "Epoch 113: Loss 20.265198259614408\n",
      "Memory=0.31 GB\n",
      "Epoch 114: Loss 20.302498585544527\n",
      "Memory=0.31 GB\n",
      "Epoch 115: Loss 20.443032236769795\n",
      "Memory=0.31 GB\n",
      "Epoch 116: Loss 20.24631005898118\n",
      "Memory=0.31 GB\n",
      "Epoch 117: Loss 20.384659205563366\n",
      "Memory=0.31 GB\n",
      "Epoch 118: Loss 20.055797955021262\n",
      "Memory=0.31 GB\n",
      "Epoch 119: Loss 20.338626069948077\n",
      "Memory=0.31 GB\n",
      "Epoch 120: Loss 20.15731222461909\n",
      "Memory=0.31 GB\n",
      "Epoch 121: Loss 20.19162210263312\n",
      "Memory=0.31 GB\n",
      "Epoch 122: Loss 20.092167180031538\n",
      "Memory=0.31 GB\n",
      "Epoch 123: Loss 20.125549093820155\n",
      "Memory=0.31 GB\n",
      "Epoch 124: Loss 20.45605869498104\n",
      "Memory=0.31 GB\n",
      "Epoch 125: Loss 20.049195863306522\n",
      "Memory=0.31 GB\n",
      "Epoch 126: Loss 20.26548869535327\n",
      "Memory=0.31 GB\n",
      "Epoch 127: Loss 20.24974208511412\n",
      "Memory=0.31 GB\n",
      "Epoch 128: Loss 20.04909182433039\n",
      "Memory=0.31 GB\n",
      "Epoch 129: Loss 20.494754118844867\n",
      "Memory=0.31 GB\n",
      "Epoch 130: Loss 20.313735843636096\n",
      "Memory=0.31 GB\n",
      "Epoch 131: Loss 20.182771635241807\n",
      "Memory=0.31 GB\n",
      "Epoch 132: Loss 19.92948185186833\n",
      "Memory=0.31 GB\n",
      "Epoch 133: Loss 20.080540754832327\n",
      "Memory=0.31 GB\n",
      "Epoch 134: Loss 20.309728362597525\n",
      "Memory=0.31 GB\n",
      "Epoch 135: Loss 20.33637075405568\n",
      "Memory=0.31 GB\n",
      "Epoch 136: Loss 20.20843814779073\n",
      "Memory=0.31 GB\n",
      "Epoch 137: Loss 20.279201711528003\n",
      "Memory=0.31 GB\n",
      "Epoch 138: Loss 20.00752578023821\n",
      "Memory=0.31 GB\n",
      "Epoch 139: Loss 20.109511903487146\n",
      "Memory=0.31 GB\n",
      "Epoch 140: Loss 19.91963751334697\n",
      "Memory=0.31 GB\n",
      "Epoch 141: Loss 19.970130416564643\n",
      "Memory=0.31 GB\n",
      "Epoch 142: Loss 20.41399470437318\n",
      "Memory=0.31 GB\n",
      "Epoch 143: Loss 20.0544613013044\n",
      "Memory=0.31 GB\n",
      "Epoch 144: Loss 19.95376692339778\n",
      "Memory=0.31 GB\n",
      "Epoch 145: Loss 19.93422194942832\n",
      "Memory=0.31 GB\n",
      "Epoch 146: Loss 20.053752405568957\n",
      "Memory=0.31 GB\n",
      "Epoch 147: Loss 19.85890586487949\n",
      "Memory=0.31 GB\n",
      "Epoch 148: Loss 20.164641490206122\n",
      "Memory=0.31 GB\n",
      "Epoch 149: Loss 20.14474624209106\n",
      "Memory=0.31 GB\n",
      "Epoch 150: Loss 20.10576367843896\n",
      "Memory=0.31 GB\n",
      "Epoch 151: Loss 19.99707160051912\n",
      "Memory=0.31 GB\n",
      "Epoch 152: Loss 20.01536822412163\n",
      "Memory=0.31 GB\n",
      "Epoch 153: Loss 20.22771307453513\n",
      "Memory=0.31 GB\n",
      "Epoch 154: Loss 20.0823892429471\n",
      "Memory=0.31 GB\n",
      "Epoch 155: Loss 20.044826159253716\n",
      "Memory=0.31 GB\n",
      "Epoch 156: Loss 20.038151120766997\n",
      "Memory=0.31 GB\n",
      "Epoch 157: Loss 20.187452950514853\n",
      "Memory=0.31 GB\n",
      "Epoch 158: Loss 20.27757255360484\n",
      "Memory=0.31 GB\n",
      "Epoch 159: Loss 19.915003903210163\n",
      "Memory=0.31 GB\n",
      "Epoch 160: Loss 19.940578364767134\n",
      "Memory=0.31 GB\n",
      "Epoch 161: Loss 19.957856410183012\n",
      "Memory=0.31 GB\n",
      "Epoch 162: Loss 19.760225044563413\n",
      "Memory=0.31 GB\n",
      "Epoch 163: Loss 20.101485705003142\n",
      "Memory=0.31 GB\n",
      "Epoch 164: Loss 20.009051802568138\n",
      "Memory=0.31 GB\n",
      "Epoch 165: Loss 19.927990614436567\n",
      "Memory=0.31 GB\n",
      "Epoch 166: Loss 19.971983023919165\n",
      "Memory=0.31 GB\n",
      "Epoch 167: Loss 19.8915744619444\n",
      "Memory=0.31 GB\n",
      "Epoch 168: Loss 19.90182483009994\n",
      "Memory=0.31 GB\n",
      "Epoch 169: Loss 19.949959362857044\n",
      "Memory=0.31 GB\n",
      "Epoch 170: Loss 19.980387066490948\n",
      "Memory=0.31 GB\n",
      "Epoch 171: Loss 19.997423687949777\n",
      "Memory=0.31 GB\n",
      "Epoch 172: Loss 19.83616689965129\n",
      "Memory=0.31 GB\n",
      "Epoch 173: Loss 20.258630621246994\n",
      "Memory=0.31 GB\n",
      "Epoch 174: Loss 19.984432889148593\n",
      "Memory=0.31 GB\n",
      "Epoch 175: Loss 19.96100654453039\n",
      "Memory=0.31 GB\n",
      "Epoch 176: Loss 19.68872830271721\n",
      "Memory=0.31 GB\n",
      "Epoch 177: Loss 19.919823887757957\n",
      "Memory=0.31 GB\n",
      "Epoch 178: Loss 19.994005457498133\n",
      "Memory=0.31 GB\n",
      "Epoch 179: Loss 19.81700189691037\n",
      "Memory=0.31 GB\n",
      "Epoch 180: Loss 19.77843782864511\n",
      "Memory=0.31 GB\n",
      "Epoch 181: Loss 20.06049312464893\n",
      "Memory=0.31 GB\n",
      "Epoch 182: Loss 19.82037102151662\n",
      "Memory=0.31 GB\n",
      "Epoch 183: Loss 19.878635920584202\n",
      "Memory=0.31 GB\n",
      "Epoch 184: Loss 20.008131315000355\n",
      "Memory=0.31 GB\n",
      "Epoch 185: Loss 20.010548340156674\n",
      "Memory=0.31 GB\n",
      "Epoch 186: Loss 19.928079328499734\n",
      "Memory=0.31 GB\n",
      "Epoch 187: Loss 20.15784587804228\n",
      "Memory=0.31 GB\n",
      "Epoch 188: Loss 19.666204823181033\n",
      "Memory=0.31 GB\n",
      "Epoch 189: Loss 19.756043998524547\n",
      "Memory=0.31 GB\n",
      "Epoch 190: Loss 19.998050414025784\n",
      "Memory=0.31 GB\n",
      "Epoch 191: Loss 19.872867746278644\n",
      "Memory=0.31 GB\n",
      "Epoch 192: Loss 20.199047593399882\n",
      "Memory=0.31 GB\n",
      "Epoch 193: Loss 19.77173162251711\n",
      "Memory=0.31 GB\n",
      "Epoch 194: Loss 19.968571769073606\n",
      "Memory=0.31 GB\n",
      "Epoch 195: Loss 19.89707442652434\n",
      "Memory=0.31 GB\n",
      "Epoch 196: Loss 19.727975401096046\n",
      "Memory=0.31 GB\n",
      "Epoch 197: Loss 19.94204950798303\n",
      "Memory=0.31 GB\n",
      "Epoch 198: Loss 19.951961547136307\n",
      "Memory=0.31 GB\n",
      "Epoch 199: Loss 19.90377064794302\n",
      "Memory=0.31 GB\n",
      "Epoch 200: Loss 19.83771526068449\n",
      "Memory=0.31 GB\n",
      "Epoch 201: Loss 19.68710998725146\n",
      "Memory=0.31 GB\n",
      "Epoch 202: Loss 19.768558183684945\n",
      "Memory=0.31 GB\n",
      "Epoch 203: Loss 19.900979527272284\n",
      "Memory=0.31 GB\n",
      "Epoch 204: Loss 19.800904014147818\n",
      "Memory=0.31 GB\n",
      "Epoch 205: Loss 19.895993585698307\n",
      "Memory=0.31 GB\n",
      "Epoch 206: Loss 19.893822565674782\n",
      "Memory=0.31 GB\n",
      "Epoch 207: Loss 19.86198219936341\n",
      "Memory=0.31 GB\n",
      "Epoch 208: Loss 19.580277744680643\n",
      "Memory=0.31 GB\n",
      "Epoch 209: Loss 19.52856584638357\n",
      "Memory=0.31 GB\n",
      "Epoch 210: Loss 19.8501934427768\n",
      "Memory=0.31 GB\n",
      "Epoch 211: Loss 19.85607458744198\n",
      "Memory=0.31 GB\n",
      "Epoch 212: Loss 19.894869722425938\n",
      "Memory=0.31 GB\n",
      "Epoch 213: Loss 19.802723742090166\n",
      "Memory=0.31 GB\n",
      "Epoch 214: Loss 19.788934824056923\n",
      "Memory=0.31 GB\n",
      "Epoch 215: Loss 19.686403261497617\n",
      "Memory=0.31 GB\n",
      "Epoch 216: Loss 19.950313623063266\n",
      "Memory=0.31 GB\n",
      "Epoch 217: Loss 19.843521684408188\n",
      "Memory=0.31 GB\n",
      "Epoch 218: Loss 19.795238764956594\n",
      "Memory=0.31 GB\n",
      "Epoch 219: Loss 19.852641607634723\n",
      "Memory=0.31 GB\n",
      "Epoch 220: Loss 19.905341744422913\n",
      "Memory=0.31 GB\n",
      "Epoch 221: Loss 19.694450526498258\n",
      "Memory=0.31 GB\n",
      "Epoch 222: Loss 19.784852322191\n",
      "Memory=0.31 GB\n",
      "Epoch 223: Loss 19.75322063267231\n",
      "Memory=0.31 GB\n",
      "Epoch 224: Loss 19.800441500730813\n",
      "Memory=0.31 GB\n",
      "Epoch 225: Loss 19.670500715263188\n",
      "Memory=0.31 GB\n",
      "Epoch 226: Loss 19.790680137462914\n",
      "Memory=0.31 GB\n",
      "Epoch 227: Loss 19.746940035372972\n",
      "Memory=0.31 GB\n",
      "Epoch 228: Loss 19.698514802381396\n",
      "Memory=0.31 GB\n",
      "Epoch 229: Loss 19.696151476353407\n",
      "Memory=0.31 GB\n",
      "Epoch 230: Loss 19.865195185877383\n",
      "Memory=0.31 GB\n",
      "Epoch 231: Loss 19.56914297863841\n",
      "Memory=0.31 GB\n",
      "Epoch 232: Loss 19.649236587807536\n",
      "Memory=0.31 GB\n",
      "Epoch 233: Loss 19.667730981484056\n",
      "Memory=0.31 GB\n",
      "Epoch 234: Loss 19.771058963611722\n",
      "Memory=0.31 GB\n",
      "Epoch 235: Loss 19.81704157963395\n",
      "Memory=0.31 GB\n",
      "Epoch 236: Loss 19.815079422667623\n",
      "Memory=0.31 GB\n",
      "Epoch 237: Loss 19.756399621255696\n",
      "Memory=0.31 GB\n",
      "Epoch 238: Loss 19.89805423282087\n",
      "Memory=0.31 GB\n",
      "Epoch 239: Loss 19.7198487771675\n",
      "Memory=0.31 GB\n",
      "Epoch 240: Loss 19.731303692795336\n",
      "Memory=0.31 GB\n",
      "Epoch 241: Loss 19.75609938427806\n",
      "Memory=0.31 GB\n",
      "Epoch 242: Loss 19.84922701213509\n",
      "Memory=0.31 GB\n",
      "Epoch 243: Loss 19.918522622436285\n",
      "Memory=0.31 GB\n",
      "Epoch 244: Loss 19.686906474642456\n",
      "Memory=0.31 GB\n",
      "Epoch 245: Loss 19.70240127015859\n",
      "Memory=0.31 GB\n",
      "Epoch 246: Loss 19.674508198164403\n",
      "Memory=0.31 GB\n",
      "Epoch 247: Loss 19.976531906984746\n",
      "Memory=0.31 GB\n",
      "Epoch 248: Loss 19.843988514505327\n",
      "Memory=0.31 GB\n",
      "Epoch 249: Loss 19.755463922396302\n",
      "Memory=0.31 GB\n",
      "Epoch 250: Loss 19.617375097237527\n",
      "Memory=0.31 GB\n",
      "Epoch 251: Loss 19.50932868383825\n",
      "Memory=0.31 GB\n",
      "Epoch 252: Loss 19.63652665540576\n",
      "Memory=0.31 GB\n",
      "Epoch 253: Loss 19.657731148414314\n",
      "Memory=0.31 GB\n",
      "Epoch 254: Loss 19.62452032417059\n",
      "Memory=0.31 GB\n",
      "Epoch 255: Loss 19.591162010096014\n",
      "Memory=0.31 GB\n",
      "Epoch 256: Loss 19.57494728639722\n",
      "Memory=0.31 GB\n",
      "Epoch 257: Loss 19.569750228896737\n",
      "Memory=0.31 GB\n",
      "Epoch 258: Loss 19.603509346023202\n",
      "Memory=0.31 GB\n",
      "Epoch 259: Loss 19.406356401741505\n",
      "Memory=0.31 GB\n",
      "Epoch 260: Loss 19.645870937034488\n",
      "Memory=0.31 GB\n",
      "Epoch 261: Loss 19.615918627008796\n",
      "Memory=0.31 GB\n",
      "Epoch 262: Loss 19.427572931163013\n",
      "Memory=0.31 GB\n",
      "Epoch 263: Loss 19.559313658624887\n",
      "Memory=0.31 GB\n",
      "Epoch 264: Loss 19.766108508221805\n",
      "Memory=0.31 GB\n",
      "Epoch 265: Loss 19.752889895811677\n",
      "Memory=0.31 GB\n",
      "Epoch 266: Loss 19.616586703807116\n",
      "Memory=0.31 GB\n",
      "Epoch 267: Loss 19.60910908691585\n",
      "Memory=0.31 GB\n",
      "Epoch 268: Loss 19.69713577441871\n",
      "Memory=0.31 GB\n",
      "Epoch 269: Loss 19.597678368911147\n",
      "Memory=0.31 GB\n",
      "Epoch 270: Loss 19.651077480986714\n",
      "Memory=0.31 GB\n",
      "Epoch 271: Loss 19.58057527989149\n",
      "Memory=0.31 GB\n",
      "Epoch 272: Loss 19.49177651759237\n",
      "Memory=0.31 GB\n",
      "Epoch 273: Loss 19.66971409972757\n",
      "Memory=0.31 GB\n",
      "Epoch 274: Loss 19.534091177396476\n",
      "Memory=0.31 GB\n",
      "Epoch 275: Loss 19.594601025804877\n",
      "Memory=0.31 GB\n",
      "Epoch 276: Loss 19.31107542477548\n",
      "Memory=0.31 GB\n",
      "Epoch 277: Loss 19.635149103589356\n",
      "Memory=0.31 GB\n",
      "Epoch 278: Loss 19.584376885555685\n",
      "Memory=0.31 GB\n",
      "Epoch 279: Loss 19.471160230226815\n",
      "Memory=0.31 GB\n",
      "Epoch 280: Loss 19.597626509144902\n",
      "Memory=0.31 GB\n",
      "Epoch 281: Loss 19.747409065254033\n",
      "Memory=0.31 GB\n",
      "Epoch 282: Loss 19.537328325212002\n",
      "Memory=0.31 GB\n",
      "Epoch 283: Loss 19.519662006758153\n",
      "Memory=0.31 GB\n",
      "Epoch 284: Loss 19.553928525187075\n",
      "Memory=0.31 GB\n",
      "Epoch 285: Loss 19.47761163301766\n",
      "Memory=0.31 GB\n",
      "Epoch 286: Loss 19.77855509147048\n",
      "Memory=0.31 GB\n",
      "Epoch 287: Loss 19.40494004264474\n",
      "Memory=0.31 GB\n",
      "Epoch 288: Loss 19.613676871173084\n",
      "Memory=0.31 GB\n",
      "Epoch 289: Loss 19.611631109379232\n",
      "Memory=0.31 GB\n",
      "Epoch 290: Loss 19.511812824755907\n",
      "Memory=0.31 GB\n",
      "Epoch 291: Loss 19.773810980841517\n",
      "Memory=0.31 GB\n",
      "Epoch 292: Loss 19.49125501140952\n",
      "Memory=0.31 GB\n",
      "Epoch 293: Loss 19.82557186111808\n",
      "Memory=0.31 GB\n",
      "Epoch 294: Loss 19.59673465974629\n",
      "Memory=0.31 GB\n",
      "Epoch 295: Loss 19.50205310434103\n",
      "Memory=0.31 GB\n",
      "Epoch 296: Loss 19.648738050833344\n",
      "Memory=0.31 GB\n",
      "Epoch 297: Loss 19.53383651562035\n",
      "Memory=0.31 GB\n",
      "Epoch 298: Loss 19.46484947670251\n",
      "Memory=0.31 GB\n",
      "Epoch 299: Loss 19.44170314911753\n",
      "Memory=0.31 GB\n",
      "Epoch 300: Loss 19.579387797042727\n",
      "Memory=0.31 GB\n",
      "Epoch 301: Loss 19.644325654022396\n",
      "Memory=0.31 GB\n",
      "Epoch 302: Loss 19.519517204724252\n",
      "Memory=0.31 GB\n",
      "Epoch 303: Loss 19.734579523094\n",
      "Memory=0.31 GB\n",
      "Epoch 304: Loss 19.471965121105313\n",
      "Memory=0.31 GB\n",
      "Epoch 305: Loss 19.2751679494977\n",
      "Memory=0.31 GB\n",
      "Epoch 306: Loss 19.663617563433945\n",
      "Memory=0.31 GB\n",
      "Epoch 307: Loss 19.521284200251102\n",
      "Memory=0.31 GB\n",
      "Epoch 308: Loss 19.657354697585106\n",
      "Memory=0.31 GB\n",
      "Epoch 309: Loss 19.346970955841243\n",
      "Memory=0.31 GB\n",
      "Epoch 310: Loss 19.455363189801574\n",
      "Memory=0.31 GB\n",
      "Epoch 311: Loss 19.259642706252635\n",
      "Memory=0.31 GB\n",
      "Epoch 312: Loss 19.421163744293153\n",
      "Memory=0.31 GB\n",
      "Epoch 313: Loss 19.411608727648854\n",
      "Memory=0.31 GB\n",
      "Epoch 314: Loss 19.711747863329947\n",
      "Memory=0.31 GB\n",
      "Epoch 315: Loss 19.314325401559472\n",
      "Memory=0.31 GB\n",
      "Epoch 316: Loss 19.492181862704456\n",
      "Memory=0.31 GB\n",
      "Epoch 317: Loss 19.407419992610812\n",
      "Memory=0.31 GB\n",
      "Epoch 318: Loss 19.591764794662595\n",
      "Memory=0.31 GB\n",
      "Epoch 319: Loss 19.47706013172865\n",
      "Memory=0.31 GB\n",
      "Epoch 320: Loss 19.390994355082512\n",
      "Memory=0.31 GB\n",
      "Epoch 321: Loss 19.59895436372608\n",
      "Memory=0.31 GB\n",
      "Epoch 322: Loss 19.389612777158618\n",
      "Memory=0.31 GB\n",
      "Epoch 323: Loss 19.632225620560348\n",
      "Memory=0.31 GB\n",
      "Epoch 324: Loss 19.497835277579725\n",
      "Memory=0.31 GB\n",
      "Epoch 325: Loss 19.601451579481363\n",
      "Memory=0.31 GB\n",
      "Epoch 326: Loss 19.555007806047797\n",
      "Memory=0.31 GB\n",
      "Epoch 327: Loss 19.38581469655037\n",
      "Memory=0.31 GB\n",
      "Epoch 328: Loss 19.677434130571783\n",
      "Memory=0.31 GB\n",
      "Epoch 329: Loss 19.268819069489837\n",
      "Memory=0.31 GB\n",
      "Epoch 330: Loss 19.469723504036665\n",
      "Memory=0.31 GB\n",
      "Epoch 331: Loss 19.466229694895446\n",
      "Memory=0.31 GB\n",
      "Epoch 332: Loss 19.523140657693148\n",
      "Memory=0.31 GB\n",
      "Epoch 333: Loss 19.403662398457527\n",
      "Memory=0.31 GB\n",
      "Epoch 334: Loss 19.558268766850233\n",
      "Memory=0.31 GB\n",
      "Epoch 335: Loss 19.594056303612888\n",
      "Memory=0.31 GB\n",
      "Epoch 336: Loss 19.36237506289035\n",
      "Memory=0.31 GB\n",
      "Epoch 337: Loss 19.56026856508106\n",
      "Memory=0.31 GB\n",
      "Epoch 338: Loss 19.51715434063226\n",
      "Memory=0.31 GB\n",
      "Epoch 339: Loss 19.560833813622594\n",
      "Memory=0.31 GB\n",
      "Epoch 340: Loss 19.378991778939962\n",
      "Memory=0.31 GB\n",
      "Epoch 341: Loss 19.42968958057463\n",
      "Memory=0.31 GB\n",
      "Epoch 342: Loss 19.60802732501179\n",
      "Memory=0.31 GB\n",
      "Epoch 343: Loss 19.61728122085333\n",
      "Memory=0.31 GB\n",
      "Epoch 344: Loss 19.58974396623671\n",
      "Memory=0.31 GB\n",
      "Epoch 345: Loss 19.589896959252656\n",
      "Memory=0.31 GB\n",
      "Epoch 346: Loss 19.48157336562872\n",
      "Memory=0.31 GB\n",
      "Epoch 347: Loss 19.327144449576735\n",
      "Memory=0.31 GB\n",
      "Epoch 348: Loss 19.47860339190811\n",
      "Memory=0.31 GB\n",
      "Epoch 349: Loss 19.411684449762106\n",
      "Memory=0.31 GB\n",
      "Epoch 350: Loss 19.383586143143475\n",
      "Memory=0.31 GB\n",
      "Epoch 351: Loss 19.457589984871447\n",
      "Memory=0.31 GB\n",
      "Epoch 352: Loss 19.445532478392124\n",
      "Memory=0.31 GB\n",
      "Epoch 353: Loss 19.38691110443324\n",
      "Memory=0.31 GB\n",
      "Epoch 354: Loss 19.497867422178388\n",
      "Memory=0.31 GB\n",
      "Epoch 355: Loss 19.405473829247057\n",
      "Memory=0.31 GB\n",
      "Epoch 356: Loss 19.247852068394423\n",
      "Memory=0.31 GB\n",
      "Epoch 357: Loss 19.47339778393507\n",
      "Memory=0.31 GB\n",
      "Epoch 358: Loss 19.486958757042885\n",
      "Memory=0.31 GB\n",
      "Epoch 359: Loss 19.268475686199963\n",
      "Memory=0.31 GB\n",
      "Epoch 360: Loss 19.386185464449227\n",
      "Memory=0.31 GB\n",
      "Epoch 361: Loss 19.35576735250652\n",
      "Memory=0.31 GB\n",
      "Epoch 362: Loss 19.433470693416893\n",
      "Memory=0.31 GB\n",
      "Epoch 363: Loss 19.418837408535182\n",
      "Memory=0.31 GB\n",
      "Epoch 364: Loss 19.444119271822274\n",
      "Memory=0.31 GB\n",
      "Epoch 365: Loss 19.35237443447113\n",
      "Memory=0.31 GB\n",
      "Epoch 366: Loss 19.553187225945294\n",
      "Memory=0.31 GB\n",
      "Epoch 367: Loss 19.57390535250306\n",
      "Memory=0.31 GB\n",
      "Epoch 368: Loss 19.377648594789207\n",
      "Memory=0.31 GB\n",
      "Epoch 369: Loss 19.45458565838635\n",
      "Memory=0.31 GB\n",
      "Epoch 370: Loss 19.598631146363914\n",
      "Memory=0.31 GB\n",
      "Epoch 371: Loss 19.15375831630081\n",
      "Memory=0.31 GB\n",
      "Epoch 372: Loss 19.36342388857156\n",
      "Memory=0.31 GB\n",
      "Epoch 373: Loss 19.472545188851655\n",
      "Memory=0.31 GB\n",
      "Epoch 374: Loss 19.43662492465228\n",
      "Memory=0.31 GB\n",
      "Epoch 375: Loss 19.35708173085004\n",
      "Memory=0.31 GB\n",
      "Epoch 376: Loss 19.399268914945424\n",
      "Memory=0.31 GB\n",
      "Epoch 377: Loss 19.532543933019042\n",
      "Memory=0.31 GB\n",
      "Epoch 378: Loss 19.376071359962225\n",
      "Memory=0.31 GB\n",
      "Epoch 379: Loss 19.35654908698052\n",
      "Memory=0.31 GB\n",
      "Epoch 380: Loss 19.179142368026078\n",
      "Memory=0.31 GB\n",
      "Epoch 381: Loss 19.505714951083064\n",
      "Memory=0.31 GB\n",
      "Epoch 382: Loss 19.589330635033548\n",
      "Memory=0.31 GB\n",
      "Epoch 383: Loss 19.29982915893197\n",
      "Memory=0.31 GB\n",
      "Epoch 384: Loss 19.539659118279815\n",
      "Memory=0.31 GB\n",
      "Epoch 385: Loss 19.200256696902215\n",
      "Memory=0.31 GB\n",
      "Epoch 386: Loss 19.30806721560657\n",
      "Memory=0.31 GB\n",
      "Epoch 387: Loss 19.268472135066986\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[155], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      3\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# shape [B, 1, 28, 28]\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# shape [B, 1]\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\luxy6\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\luxy6\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\luxy6\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\luxy6\\miniconda3\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\luxy6\\miniconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\luxy6\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luxy6\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luxy6\\miniconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luxy6\\miniconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luxy6\\miniconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:920\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    918\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    919\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, leading to division by zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for batch_idx, (x0, c) in enumerate(train_loader):\n",
    "        x0 = x0.to(device)  # shape [B, 1, 28, 28]\n",
    "        c = c.to(device) # shape [B, 1]\n",
    "\n",
    "        # Sample random timestep for each image in the batch\n",
    "        t = torch.randint(0, T, (x0.size(0),), device=device).long()\n",
    "\n",
    "        # Compute corresponding alpha cumulative product for each t\n",
    "        alpha_bar_t = alphas_cumprod[t].to(device).view(-1, 1, 1, 1)\n",
    "\n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(x0)\n",
    "\n",
    "        # Create noisy image x_t\n",
    "        x_t = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * noise\n",
    "\n",
    "        # Predict noise using model\n",
    "        # Normalize timestep to a range or use embedding directly\n",
    "        t_normalized = t.unsqueeze(-1).float() / T\n",
    "\n",
    "        pred_noise = model(x_t, t_normalized, c)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = mse_loss(pred_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Memory={torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB\")\n",
    "    print(f\"Epoch {epoch}: Loss {running_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, T, shape):\n",
    "    model.eval()\n",
    "    x = torch.randn(shape, device=device)  # start from pure noise\n",
    "    for t in reversed(range(T)):\n",
    "        # Create tensor for current timestep\n",
    "        t_tensor = torch.full((shape[0], 1), t, device=device).float() / T\n",
    "        c = torch.tensor([3]).type(torch.int64).to(device)\n",
    "        # Predict noise\n",
    "        pred_noise = model(x, t_tensor, c)\n",
    "        # Compute coefficients\n",
    "        alpha = alphas[t]\n",
    "        alpha_bar = alphas_cumprod[t]\n",
    "        alpha_bar_prev = alphas_cumprod[t - 1]\n",
    "        beta = betas[t]\n",
    "\n",
    "        # Predict the previous step x_{t-1}\n",
    "        # Simplified update for demonstration; actual update may require more careful implementation\n",
    "        x = (1/torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_bar)) * pred_noise)\n",
    "        # Optionally add noise if t > 0\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma = 1.05 * torch.sqrt(beta) * (1. - alpha_bar_prev) /  (1. - alpha_bar)\n",
    "            x += 0.9 * sigma * noise\n",
    "\n",
    "    return x\n",
    "\n",
    "# Generate new samples\n",
    "samples = sample(model, T, shape=(16, 1, 28, 28)).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Each sample is shape [1, 28, 28], so we take the first channel\n",
    "    img = samples[i][0]  \n",
    "    # Rescale the image values from normalization range if needed (e.g., from [-1,1] to [0,1])\n",
    "    img = (img + 1) / 2  \n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
